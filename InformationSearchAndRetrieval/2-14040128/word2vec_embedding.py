# -*- coding: utf-8 -*-
"""Word2Vec_Embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/akaravi/NTK.Education.Practice/blob/main/InformationSearchAndRetrieval/2-14040128/Word2Vec_Embedding.ipynb

Implement Word2Vec

In this assignment, you are required to implement the `Word2Vec` class from scratch using Python.

Please complete the methods inside the `Word2Vec` class:
- `__init__`: Initialize weights
- `softmax`: Impliment softmax activation function
- `train`: Loop over the data and optimize your model

You may use NumPy, but **do not** use any external word embedding libraries like Gensim or Torch for this task.

Good luck!

# Skip Gram Implementation
"""

import re
import numpy as np
from collections import defaultdict
import random
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

class Vocabulary:
    def __init__(self):
        self.word2idx = {}
        self.idx2word = {}
        self.word_count = defaultdict(int)
        self.total_words = 0
        self.vocab_size = 0

    def build_vocab(self, sentences, min_count=2):
        # Count words
        for word in sentences:
            self.word_count[word] += 1

        # Create word2idx and idx2word mapping
        idx = 0
        for word, count in self.word_count.items():
            if count >= min_count:
              self.word2idx.update({word: idx})
              idx += 1

        # self.word2idx = {word: idx for idx, (word, count) in enumerate(self.word_count.items()) if count >= min_count}
        self.idx2word = {idx: word for word, idx in self.word2idx.items()}
        self.vocab_size = len(self.word2idx)
        self.total_words = sum([count for word, count in self.word_count.items() if count >= min_count])

    def word_to_index(self, word):
        return self.word2idx.get(word, -1)

    def index_to_word(self, index):
        return self.idx2word.get(index, None)

def generate_training_data(vocab, sentences, window_size=2):
    training_data = []
    sentence_indices = [vocab.word_to_index(word) for word in sentences if vocab.word_to_index(word) != -1]

    for center_idx, center_word in enumerate(sentence_indices):
        context_start = max(0, center_idx - window_size)
        context_end = min(len(sentence_indices), center_idx + window_size + 1)

        for context_idx in range(context_start, context_end):
            if context_idx != center_idx:
                context_word = sentence_indices[context_idx]
                training_data.append((center_word, context_word))

    return np.array(training_data)

"""شروع تمرین
######################################
######################################
######################################
"""

class Word2Vec:
    def __init__(self, vocab_size, embed_size=100, learning_rate=0.001):
        #Start
        # سایز زا مشخص می کنم
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.learning_rate = learning_rate

        # مقداردهی اولیه ماتریس‌های وزن
        #ورودی مخفی
        self.W = np.random.uniform(-0.5, 0.5, (vocab_size, embed_size))
        #خروجی
        self.W_prime = np.random.uniform(-0.5, 0.5, (embed_size, vocab_size))
        #End




    def softmax(self, x):
        #Start
        xMax=np.max(x)
        retOut = np.exp(x - xMax)
        return retOut / np.sum(retOut)
        #ENd



    def train(self, training_data, epochs=1000):
        #Start
        #تعداد تکرار برای بررسی نتججه
        for epochNumber in range(epochs):
            myloss = 0
            #بررسی وزن ها
            for c_word, context_word in training_data:
                hValue = self.W[c_word]
                dispacher = np.dot(hValue, self.W_prime)
                y_pred = self.softmax(dispacher)

                #encoding
                y_true = np.zeros(self.vocab_size)
                y_true[context_word] = 1

                # مجاسبه خطا
                er = y_pred - y_true

                # برروز رسانی وزن ها
                self.W_prime -= self.learning_rate * np.outer(hValue, er)
                self.W[c_word] -= self.learning_rate * np.dot(self.W_prime, er)

                # محاسبه looss
                myloss -= np.log(y_pred[context_word])

            if epochNumber % 100 == 0:
                print(f'Epoch Count {epochNumber}, Calculat Loss: {myloss}')

"""######################################
######################################
######################################
پایان تمرین
"""

# Reading the Persian stopwords from the file
from google.colab import files
# بارگذاری فایل
uploaded = files.upload()
# نام فایل بارگذاری شده
filename = list(uploaded.keys())[0]

with open(filename, 'r', encoding='utf-8') as f:
    stopwords = set(f.read().splitlines())  # Using a set for faster lookup

# Reading the Persian stopwords from the file
#with open('persian.txt', 'r', encoding='utf-8') as f:
#    stopwords = set(f.read().splitlines())  # Using a set for faster lookup

# Sample Persian text
text = "ملکه و زن ها در کنار همسران و خانواده خود یعنی شاه و مرد ها در یک سرزمین پهناور زندگی می‌کردند شاه همیشه به مرد ها تذکر میداد که قدرت در اتحاد مرد ها و شاه نهفته است و در این قلمرو ملکه به زن ها یادآوری می‌کرد که همبستگی زن ها و ملکه مهم است و در این داستان هر مرد که نزد شاه یا زن که نزد ملکه می‌آمد از آنها حکم می‌گرفت تا به دیگران کمک کنند شاه عادل و قادر بود و ملکه خردمند و زیبا و هر مرد که از حکمت شاه یا زن که از عدالت ملکه راضی نبود نزد آنها می‌رفت تا شکایت خود را مطرح کند شاه و مرد و ملکه و زن در کنار هم بودند و هیچ کس از شاه یا ملکه نمی‌ترسید شاه همیشه به زبان میاورد که مرد ها باید به یکدیگر کمک کنند و ملکه تأکید داشتند زن ها هم باید متحد باشند"

# Tokenizing the text (you can modify the tokenizer if needed)
words = text.split()

# Removing stopwords
filtered_text = [word for word in words if word not in stopwords]

# Joining the words back into a sentence
cleaned_text = ' '.join(filtered_text)

print(cleaned_text)

# wiki_dump_path = 'enwiki-latest-pages-articles.xml.bz2'  # Path to your Wikipedia dump file

# # Load and preprocess the dataset
# sentences = list(load_wiki_data(wiki_dump_path))

# Build the vocabulary
vocab = Vocabulary()
vocab.build_vocab(cleaned_text.split(' '))

# Generate training data
training_data = generate_training_data(vocab, cleaned_text.split(' '))

# Initialize and train Word2Vec model
word2vec_model = Word2Vec(vocab.vocab_size)
word2vec_model.train(training_data, epochs=1000)

"""Defines a function to retrieve the word embedding for a given word from a Word2Vec model. If the word exists in the vocabulary, its corresponding vector is returned; otherwise, None is returned."""

def get_word_embedding(word, vocab, model):
    word_idx = vocab.word_to_index(word)
    if word_idx != -1:
        return model.W[word_idx]
    else:
        return None

embedding = get_word_embedding("مرد", vocab, word2vec_model)
print(embedding)