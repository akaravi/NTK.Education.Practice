{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N54hBSP_micD"
      },
      "source": [
        "Implement Word2Vec\n",
        "\n",
        "In this assignment, you are required to implement the `Word2Vec` class from scratch using Python.\n",
        "\n",
        "Please complete the methods inside the `Word2Vec` class:\n",
        "- `__init__`: Initialize weights\n",
        "- `softmax`: Impliment softmax activation function\n",
        "- `train`: Loop over the data and optimize your model\n",
        "\n",
        "You may use NumPy, but **do not** use any external word embedding libraries like Gensim or Torch for this task.\n",
        "\n",
        "Good luck!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUI4Ahw9loe4"
      },
      "source": [
        "# Skip Gram Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfKVbKrZcCBR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPP3YaQhcHgx"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word_count = defaultdict(int)\n",
        "        self.total_words = 0\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def build_vocab(self, sentences, min_count=2):\n",
        "        # Count words\n",
        "        for word in sentences:\n",
        "            self.word_count[word] += 1\n",
        "\n",
        "        # Create word2idx and idx2word mapping\n",
        "        idx = 0\n",
        "        for word, count in self.word_count.items():\n",
        "            if count >= min_count:\n",
        "              self.word2idx.update({word: idx})\n",
        "              idx += 1\n",
        "\n",
        "        # self.word2idx = {word: idx for idx, (word, count) in enumerate(self.word_count.items()) if count >= min_count}\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "        self.total_words = sum([count for word, count in self.word_count.items() if count >= min_count])\n",
        "\n",
        "    def word_to_index(self, word):\n",
        "        return self.word2idx.get(word, -1)\n",
        "\n",
        "    def index_to_word(self, index):\n",
        "        return self.idx2word.get(index, None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPzlftDvcKlo"
      },
      "outputs": [],
      "source": [
        "def generate_training_data(vocab, sentences, window_size=2):\n",
        "    training_data = []\n",
        "    sentence_indices = [vocab.word_to_index(word) for word in sentences if vocab.word_to_index(word) != -1]\n",
        "\n",
        "    for center_idx, center_word in enumerate(sentence_indices):\n",
        "        context_start = max(0, center_idx - window_size)\n",
        "        context_end = min(len(sentence_indices), center_idx + window_size + 1)\n",
        "\n",
        "        for context_idx in range(context_start, context_end):\n",
        "            if context_idx != center_idx:\n",
        "                context_word = sentence_indices[context_idx]\n",
        "                training_data.append((center_word, context_word))\n",
        "\n",
        "    return np.array(training_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شروع تمرین"
      ],
      "metadata": {
        "id": "g4E3wHHUsoa0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r7DO0LhcNNo"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, vocab_size, embed_size=100, learning_rate=0.001):\n",
        "        # سایز زا مشخص می کنم\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # مقداردهی اولیه ماتریس‌های وزن\n",
        "        #ورودی مخفی\n",
        "        self.W = np.random.uniform(-0.5, 0.5, (vocab_size, embed_size))\n",
        "        #خروجی\n",
        "        self.W_prime = np.random.uniform(-0.5, 0.5, (embed_size, vocab_size))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        ex = np.exp(x - np.max(x))\n",
        "        return ex / np.sum(ex)\n",
        "\n",
        "    def train(self, training_data, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            myloss = 0\n",
        "            for c_word, context_word in training_data:\n",
        "                hValue = self.W[c_word]\n",
        "                u = np.dot(hValue, self.W_prime)\n",
        "                y_pred = self.softmax(u)\n",
        "\n",
        "                #encoding\n",
        "                y_true = np.zeros(self.vocab_size)\n",
        "                y_true[context_word] = 1\n",
        "\n",
        "                # مجاسبه خطا\n",
        "                er = y_pred - y_true\n",
        "\n",
        "                # برروز رسانی وزن ها\n",
        "                self.W_prime -= self.learning_rate * np.outer(h, er)\n",
        "                self.W[c_word] -= self.learning_rate * np.dot(self.W_prime, er)\n",
        "\n",
        "                # محاسبه looss\n",
        "                myloss -= np.log(y_pred[context_word])\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Calculat Loss: {myloss}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "پایان تمرین"
      ],
      "metadata": {
        "id": "uGzupkJastbP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LlbDm1y-QTy"
      },
      "outputs": [],
      "source": [
        "# Reading the Persian stopwords from the file\n",
        "from google.colab import files\n",
        "# بارگذاری فایل\n",
        "uploaded = files.upload()\n",
        "# نام فایل بارگذاری شده\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().splitlines())  # Using a set for faster lookup\n",
        "\n",
        "# Reading the Persian stopwords from the file\n",
        "#with open('persian.txt', 'r', encoding='utf-8') as f:\n",
        "#    stopwords = set(f.read().splitlines())  # Using a set for faster lookup\n",
        "\n",
        "# Sample Persian text\n",
        "text = \"ملکه و زن ها در کنار همسران و خانواده خود یعنی شاه و مرد ها در یک سرزمین پهناور زندگی می‌کردند شاه همیشه به مرد ها تذکر میداد که قدرت در اتحاد مرد ها و شاه نهفته است و در این قلمرو ملکه به زن ها یادآوری می‌کرد که همبستگی زن ها و ملکه مهم است و در این داستان هر مرد که نزد شاه یا زن که نزد ملکه می‌آمد از آنها حکم می‌گرفت تا به دیگران کمک کنند شاه عادل و قادر بود و ملکه خردمند و زیبا و هر مرد که از حکمت شاه یا زن که از عدالت ملکه راضی نبود نزد آنها می‌رفت تا شکایت خود را مطرح کند شاه و مرد و ملکه و زن در کنار هم بودند و هیچ کس از شاه یا ملکه نمی‌ترسید شاه همیشه به زبان میاورد که مرد ها باید به یکدیگر کمک کنند و ملکه تأکید داشتند زن ها هم باید متحد باشند\"\n",
        "\n",
        "# Tokenizing the text (you can modify the tokenizer if needed)\n",
        "words = text.split()\n",
        "\n",
        "# Removing stopwords\n",
        "filtered_text = [word for word in words if word not in stopwords]\n",
        "\n",
        "# Joining the words back into a sentence\n",
        "cleaned_text = ' '.join(filtered_text)\n",
        "\n",
        "print(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkspL7i-cQ43"
      },
      "outputs": [],
      "source": [
        "# wiki_dump_path = 'enwiki-latest-pages-articles.xml.bz2'  # Path to your Wikipedia dump file\n",
        "\n",
        "# # Load and preprocess the dataset\n",
        "# sentences = list(load_wiki_data(wiki_dump_path))\n",
        "\n",
        "# Build the vocabulary\n",
        "vocab = Vocabulary()\n",
        "vocab.build_vocab(cleaned_text.split(' '))\n",
        "\n",
        "# Generate training data\n",
        "training_data = generate_training_data(vocab, cleaned_text.split(' '))\n",
        "\n",
        "# Initialize and train Word2Vec model\n",
        "word2vec_model = Word2Vec(vocab.vocab_size)\n",
        "word2vec_model.train(training_data, epochs=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THLeFGKsl-2n"
      },
      "source": [
        "Defines a function to retrieve the word embedding for a given word from a Word2Vec model. If the word exists in the vocabulary, its corresponding vector is returned; otherwise, None is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y05t33ogufIm"
      },
      "outputs": [],
      "source": [
        "def get_word_embedding(word, vocab, model):\n",
        "    word_idx = vocab.word_to_index(word)\n",
        "    if word_idx != -1:\n",
        "        return model.W[word_idx]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "embedding = get_word_embedding(\"مرد\", vocab, word2vec_model)\n",
        "print(embedding)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}