{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment: Implement Word2Vec\n",
    "\n",
    "In this assignment, you are required to implement the `Word2Vec` class from scratch using Python.\n",
    "\n",
    "Please complete the methods inside the `Word2Vec` class:\n",
    "- `__init__`: Initialize weights\n",
    "- `softmax`: Impliment softmax activation function\n",
    "- `train`: Loop over the data and optimize your model\n",
    "\n",
    "You may use NumPy, but **do not** use any external word embedding libraries like Gensim or Torch for this task.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUI4Ahw9loe4"
   },
   "source": [
    "# Skip Gram Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2427,
     "status": "ok",
     "timestamp": 1730294427529,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "UfKVbKrZcCBR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1730293574861,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "JPP3YaQhcHgx"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = defaultdict(int)\n",
    "        self.total_words = 0\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def build_vocab(self, sentences, min_count=2):\n",
    "        # Count words\n",
    "        for word in sentences:\n",
    "            self.word_count[word] += 1\n",
    "\n",
    "        # Create word2idx and idx2word mapping\n",
    "        idx = 0\n",
    "        for word, count in self.word_count.items():\n",
    "            if count >= min_count:\n",
    "              self.word2idx.update({word: idx})\n",
    "              idx += 1\n",
    "\n",
    "        # self.word2idx = {word: idx for idx, (word, count) in enumerate(self.word_count.items()) if count >= min_count}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.total_words = sum([count for word, count in self.word_count.items() if count >= min_count])\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        return self.word2idx.get(word, -1)\n",
    "\n",
    "    def index_to_word(self, index):\n",
    "        return self.idx2word.get(index, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730293576569,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "gPzlftDvcKlo"
   },
   "outputs": [],
   "source": [
    "def generate_training_data(vocab, sentences, window_size=2):\n",
    "    training_data = []\n",
    "    sentence_indices = [vocab.word_to_index(word) for word in sentences if vocab.word_to_index(word) != -1]\n",
    "\n",
    "    for center_idx, center_word in enumerate(sentence_indices):\n",
    "        context_start = max(0, center_idx - window_size)\n",
    "        context_end = min(len(sentence_indices), center_idx + window_size + 1)\n",
    "\n",
    "        for context_idx in range(context_start, context_end):\n",
    "            if context_idx != center_idx:\n",
    "                context_word = sentence_indices[context_idx]\n",
    "                training_data.append((center_word, context_word))\n",
    "\n",
    "    return np.array(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1730293580366,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "3r7DO0LhcNNo"
   },
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, vocab_size, embed_size=100, learning_rate=0.001):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.W = np.random.uniform(-0.5, 0.5, (vocab_size, embed_size))  # Input to hidden\n",
    "        self.W_prime = np.random.uniform(-0.5, 0.5, (embed_size, vocab_size))  # Hidden to output\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "\n",
    "    def train(self, training_data, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for center_word, context_word in training_data:\n",
    "                h = self.W[center_word]  # Get hidden layer (input->hidden)\n",
    "                u = np.dot(h, self.W_prime)  # Predict output\n",
    "                y_pred = self.softmax(u)\n",
    "\n",
    "                # One-hot encoding the true context word\n",
    "                y_true = np.zeros(self.vocab_size)\n",
    "                y_true[context_word] = 1\n",
    "\n",
    "                # Calculate the error\n",
    "                error = y_pred - y_true\n",
    "\n",
    "                # Update weights with gradient descent\n",
    "                self.W_prime -= self.learning_rate * np.outer(h, error)\n",
    "                self.W[center_word] -= self.learning_rate * np.dot(self.W_prime, error)\n",
    "\n",
    "                # Calculate loss (cross-entropy)\n",
    "                loss -= np.log(y_pred[context_word])\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1727106128605,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "9LlbDm1y-QTy",
    "outputId": "006b1263-0b27-4b0c-aa3a-17efd44c7f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ملکه زن همسران خانواده شاه مرد سرزمین پهناور زندگی می‌کردند شاه مرد تذکر میداد قدرت اتحاد مرد شاه نهفته قلمرو ملکه زن یادآوری می‌کرد همبستگی زن ملکه مهم داستان مرد شاه زن ملکه می‌آمد حکم می‌گرفت کمک شاه عادل قادر ملکه خردمند زیبا مرد حکمت شاه زن عدالت ملکه راضی می‌رفت شکایت مطرح شاه مرد ملکه زن شاه ملکه نمی‌ترسید شاه زبان میاورد مرد کمک ملکه تأکید زن متحد\n"
     ]
    }
   ],
   "source": [
    "# Reading the Persian stopwords from the file\n",
    "with open('persian.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())  # Using a set for faster lookup\n",
    "\n",
    "# Sample Persian text\n",
    "text = \"ملکه و زن ها در کنار همسران و خانواده خود یعنی شاه و مرد ها در یک سرزمین پهناور زندگی می‌کردند شاه همیشه به مرد ها تذکر میداد که قدرت در اتحاد مرد ها و شاه نهفته است و در این قلمرو ملکه به زن ها یادآوری می‌کرد که همبستگی زن ها و ملکه مهم است و در این داستان هر مرد که نزد شاه یا زن که نزد ملکه می‌آمد از آنها حکم می‌گرفت تا به دیگران کمک کنند شاه عادل و قادر بود و ملکه خردمند و زیبا و هر مرد که از حکمت شاه یا زن که از عدالت ملکه راضی نبود نزد آنها می‌رفت تا شکایت خود را مطرح کند شاه و مرد و ملکه و زن در کنار هم بودند و هیچ کس از شاه یا ملکه نمی‌ترسید شاه همیشه به زبان میاورد که مرد ها باید به یکدیگر کمک کنند و ملکه تأکید داشتند زن ها هم باید متحد باشند\"\n",
    "\n",
    "# Tokenizing the text (you can modify the tokenizer if needed)\n",
    "words = text.split()\n",
    "\n",
    "# Removing stopwords\n",
    "filtered_text = [word for word in words if word not in stopwords]\n",
    "\n",
    "# Joining the words back into a sentence\n",
    "cleaned_text = ' '.join(filtered_text)\n",
    "\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5749,
     "status": "ok",
     "timestamp": 1727106139433,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "NkspL7i-cQ43",
    "outputId": "54375f5c-55d3-47ce-aa4f-c52332252db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 217.34630970038413\n",
      "Epoch 100, Loss: 181.5196690584462\n",
      "Epoch 200, Loss: 181.10669945586082\n",
      "Epoch 300, Loss: 180.9694558003934\n",
      "Epoch 400, Loss: 180.89839648378222\n",
      "Epoch 500, Loss: 180.85521332091994\n",
      "Epoch 600, Loss: 180.82638982519742\n",
      "Epoch 700, Loss: 180.80589790542393\n",
      "Epoch 800, Loss: 180.7906516880482\n",
      "Epoch 900, Loss: 180.77891218264517\n"
     ]
    }
   ],
   "source": [
    "# wiki_dump_path = 'enwiki-latest-pages-articles.xml.bz2'  # Path to your Wikipedia dump file\n",
    "\n",
    "# # Load and preprocess the dataset\n",
    "# sentences = list(load_wiki_data(wiki_dump_path))\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(cleaned_text.split(' '))\n",
    "\n",
    "# Generate training data\n",
    "training_data = generate_training_data(vocab, cleaned_text.split(' '))\n",
    "\n",
    "# Initialize and train Word2Vec model\n",
    "word2vec_model = Word2Vec(vocab.vocab_size)\n",
    "word2vec_model.train(training_data, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THLeFGKsl-2n"
   },
   "source": [
    "Defines a function to retrieve the word embedding for a given word from a Word2Vec model. If the word exists in the vocabulary, its corresponding vector is returned; otherwise, None is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1727104712881,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "y05t33ogufIm",
    "outputId": "4e8911f4-4ab8-4981-b552-a5e3eb766478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28634888 -0.13093086 -0.0201545   0.05271528 -0.09334632  0.15840051\n",
      " -0.34445462  0.42237267 -0.38221831 -0.19108261  0.47323099  0.4272571\n",
      "  0.13851498 -0.22135522  0.23678668 -0.28319392  0.30872885  0.39926364\n",
      " -0.52409306 -0.22185257 -0.3983401  -0.16248012  0.2544923  -0.30862513\n",
      " -0.14724238  0.15341193  0.22308444 -0.03885356  0.50674215  0.3120602\n",
      " -0.22368227 -0.42046334  0.34820725 -0.11986774 -0.39504836 -0.23070816\n",
      " -0.40190402  0.41267271 -0.07634545 -0.19427366  0.22134564 -0.38971622\n",
      " -0.54359274  0.44488732  0.1493101  -0.42904048  0.24654613 -0.26918437\n",
      "  0.02520455 -0.3487133   0.34302321 -0.32930649 -0.26991703  0.21795934\n",
      " -0.30534359 -0.08726335  0.40263831  0.10635999 -0.3494115  -0.16635264\n",
      "  0.40776723 -0.01813031  0.21182939 -0.40908686  0.2195951   0.39119267\n",
      " -0.2490812   0.3413859   0.20336299 -0.11541427  0.00648979 -0.27482335\n",
      " -0.37028258  0.2318869   0.46906915 -0.27019587 -0.18769502 -0.00147952\n",
      " -0.31550756  0.12446    -0.17474903 -0.36052425  0.09141397  0.30095765\n",
      "  0.15295545 -0.43981975 -0.43253203  0.33633545  0.06321314 -0.30296066\n",
      " -0.45822014 -0.43344143  0.0770793  -0.32663437  0.2646347  -0.06766607\n",
      "  0.46155399 -0.03132114 -0.20568234 -0.16622358]\n"
     ]
    }
   ],
   "source": [
    "def get_word_embedding(word, vocab, model):\n",
    "    word_idx = vocab.word_to_index(word)\n",
    "    if word_idx != -1:\n",
    "        return model.W[word_idx]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "embedding = get_word_embedding(\"مرد\", vocab, word2vec_model)\n",
    "print(embedding)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPDNvIJDMFdDJ9FHHgt1zLD",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
